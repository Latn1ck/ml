\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage[russian]{babel}
\voffset=-10mm
\oddsidemargin=5mm
\evensidemargin=0mm
\textheight=235mm
\textwidth=170mm
\topmargin=-7.2mm
\newtheorem{theorem}{\hskip\parindent Теорема}%[section]
\newtheorem{definition}{\hskip\parindent Определение}%[section]
\newtheorem{corollary}{\hskip\parindent Следствие}%[section]
\newtheorem{lemma}{\hskip\parindent Лемма}%[section]
\newtheorem{remark}{\hskip\parindent Замечание}%[section]
\newtheorem{example}{\hskip\parindent Пример}%[section]
\begin{document}
	\section*{Градиентный бустинг}
	Один из ансамблевых методов обучения, основанный на построении линейной комбинации <<слабых>> алгоритмов. Название связано не с понятием \textbf{градиента}, а с \textbf{методом градиентного спуска}. Одно из преимуществ ансамблевых алгоритмов заключается в том, что построенные таким образом модели практически переобучаются гораздо меньше (если один алгоритм ошибается, то другие исправляют эту ошибку).
	
	\textbf{Гиперпараметры}: скорость обучения $\alpha$, число деревьев $m$, максимальная глубина деревьев $d$.
	\section*{Регрессия}
	Пусть $\left\{(x_i,y_i)\right\}_{i=1}^{N}$ --- обучающая выборка, каждый образец $x_i$ определяется своим вектором признаков $(x_i^{(1)},\dots, x_i^{(n)})$, $y_i\in\mathbb{R}$, $a$ --- искомый алгоритм. Функция потерь --- MSE:
	\begin{equation*}
		\mathcal{L}(a,y)=\frac{1}{N}\sum\limits_{i=1}^{N}(y_i-a(x_i))^2
	\end{equation*}

Будем строить композицию алгоритмов (деревья решений). Начнём с дерева $a_0$ глубины $0$, которое каждому образцу ставит в соответствие прогноз
\begin{equation*}
	\overline{y}:=\frac{1}{N}\sum\limits_{i=1}^{N}y_i,
\end{equation*}
затем вычислим разности
\begin{equation*}
	\widehat{y_i}:=y_i-a_1(x_i)=y_i-\overline{y}
\end{equation*}
и положим $a\equiv a_0$.

Переобозначим
\begin{equation*}
	y_i:=\widehat{y_i}
\end{equation*}
и для новых меток построим новое дерево решений $a_1$. Положим
\begin{equation*}
	a(x)=a_0(x)+\alpha\cdot a_1(x)
\end{equation*}

Будем продолжать процесс, пока $a$ не примет вид
\begin{equation*}
	a(x)=a_0(x)+\alpha\cdot a_1(x)+\dots+\alpha\cdot a_m(x)
\end{equation*}
	\section*{(бинарная) Классификация}
	Градиентный бустинг для задач классификации тесно связан с логистической регрессией.
	
	
	Пусть $\left\{(x_i,y_i)\right\}_{i=1}^{N}$ --- обучающая выборка, каждый образец $x_i$ определяется своим вектором признаков $(x_i^{(1)},\dots, x_i^{(n)})$, $y_i\in\{0,1\}$, $a$ --- искомый алгоритм, возвращающий вероятность принадлежности заданного образца классу $1$.

Будем строить композицию алгоритмов (деревья решений). Начнём с дерева $a_0$ глубины $0$, которое каждый образец классифицирует как элемент класса $1$. Затем вычислим величину
	\begin{equation*}
		\operatorname{lnodds}:=\ln\frac{\#\{y_i:y_i=1\}}{\#\{y_i:y_i=0\}}
	\end{equation*}
\begin{equation*}
	\beta_i=\operatorname{lnodds}, \quad 1\leqslant i\leqslant N
\end{equation*}
Вероятности
\begin{equation*}
	p_{0i}:=P\{y_i=1\}=\frac{e^{\beta_i}}{1+e^{\beta_i}}
\end{equation*}
и разности для каждого образца
\begin{equation*}
	\widehat{y_i}=y_i-p_{0i}
\end{equation*}

Теперь построим дерево решений $a_1$, чтобы спрогнозировать разности (следует ограничивать число листьев, на практике обычно от $8$ до $32$). Пусть $L_{11}$, $\dots$, $L_{1l_1}$ --- листья дерева $a_1$. Каждый образец $x_i$ приходит в некоторый лист $L_{1j}$. Поставим каждому листу в соответствие число
\begin{equation*}
	\gamma_{1i}=\frac{\sum\limits_{x_i\in L_{1j}}\widehat{y_i}}{\sum\limits_{x_i\in L_{1j}}p_{0i}(1-p_{0i})}
\end{equation*}
Положим
\begin{equation*}
	\beta_i:=\operatorname{lnodds}+\alpha\cdot\gamma_{1i}, \quad p_{1i}=\frac{e^{\beta_i}}{1+e^{\beta_i}}, \quad \widehat{y_i}:=y_i-p_{1i}
\end{equation*}
Затем будем строить новое дерево решений $a_2$ и повторять процедуру, пока не построим $m$ деревьев.

На выходе для каждого образца $x_i$, $1\leqslant i\leqslant N$, получим вероятность принадлежности $x_i$ к классу $1$:
\begin{equation*}
	x_i\mapsto P\{y_i=1\}.
\end{equation*}
\end{document}